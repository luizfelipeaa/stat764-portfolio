[["index.html", "A Minimal Book Example Chapter 1 Prerequisites", " A Minimal Book Example Yihui Xie 2024-05-08 Chapter 1 Prerequisites This is a sample book written in Markdown. You can use anything that Pandoc’s Markdown supports, e.g., a math equation \\(a^2 + b^2 = c^2\\). The bookdown package can be installed from CRAN or Github: install.packages(&quot;bookdown&quot;) # or the development version # devtools::install_github(&quot;rstudio/bookdown&quot;) Remember each Rmd file contains one and only one chapter, and a chapter is defined by the first-level heading #. To compile this example to PDF, you need XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): https://yihui.name/tinytex/. "],["intro.html", "Chapter 2 Introduction", " Chapter 2 Introduction You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter 2. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter 4. Figures and tables with captions will be placed in figure and table environments, respectively. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 2.1: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 2.1. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 2.1. knitr::kable( head(iris, 20), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 2.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa You can write citations, too. For example, we are using the bookdown package (Xie 2024) in this sample book, which was built on top of R Markdown and knitr (Xie 2015). References "],["literature.html", "Chapter 3 Literature", " Chapter 3 Literature Here is a review of existing methods. "],["methods.html", "Chapter 4 Methods 4.1 math example", " Chapter 4 Methods We describe our methods in this chapter. Math can be added in body using usual syntax like this 4.1 math example \\(p\\) is unknown but expected to be around 1/3. Standard error will be approximated \\[ SE = \\sqrt(\\frac{p(1-p)}{n}) \\approx \\sqrt{\\frac{1/3 (1 - 1/3)} {300}} = 0.027 \\] You can also use math in footnotes like this1. We will approximate standard error to 0.0272 where we mention \\(p = \\frac{a}{b}\\)↩︎ \\(p\\) is unknown but expected to be around 1/3. Standard error will be approximated \\[ SE = \\sqrt(\\frac{p(1-p)}{n}) \\approx \\sqrt{\\frac{1/3 (1 - 1/3)} {300}} = 0.027 \\]↩︎ "],["applications.html", "Chapter 5 Applications 5.1 Example one 5.2 Example two", " Chapter 5 Applications Some significant applications are demonstrated in this chapter. 5.1 Example one 5.2 Example two "],["final-words.html", "Chapter 6 Final Words", " Chapter 6 Final Words We have finished a nice book. "],["references.html", "References", " References "],["activity-1.html", "Chapter 7 Activity 1 7.1 Load packages 7.2 Obtain a .gpx or .csv file for your movement data. 7.3 Plot/map your movement data. I would recommend using R and/or Google earth as I demonstrated in class. 7.4 Explore your movement data. For example, are there any unique features of your data (e.g., a large change in location)? Do your data contain location error? Really try to explore your data as best as possible using the plots/maps you made in 3. 7.5 Fit a statistical or machine learning model to your movement data. Obtain predictions of your location on a fine time scale so that the estimates resemble a continuous trajectory. 7.6 Plot/map your estimated trajectory from 5. Explore your estimated trajectory as best as possible using the plots/maps. Note any unique features or shortcomings of your model. 7.7 Estimate a feature or quantity of interest from your estimated trajectory (e.g., velocity, residence time, number of contacts, etc)", " Chapter 7 Activity 1 7.1 Load packages library(sf) library(lubridate) library(mgcv) library(rpart) library(party) library(leaflet) library(patchwork) library(dplyr) library(tidyr) library(ggplot2) 7.2 Obtain a .gpx or .csv file for your movement data. 7.3 Plot/map your movement data. I would recommend using R and/or Google earth as I demonstrated in class. # Plot the movement data leaflet(data) %&gt;% addProviderTiles(providers$Esri.WorldImagery) %&gt;% addCircleMarkers( color = &quot;darkorange&quot;, fill = TRUE, fillColor = &quot;darkorange&quot;, fillOpacity = 0.8, radius = 2, popup = ~paste(&quot;Time:&quot;, format(time, &quot;%T&quot;))) 7.4 Explore your movement data. For example, are there any unique features of your data (e.g., a large change in location)? Do your data contain location error? Really try to explore your data as best as possible using the plots/maps you made in 3. # Extract coordinates and convert to a data frame coords &lt;- as.data.frame(st_coordinates(data)) # Add elevation and time coords$ele &lt;- data$ele coords$time &lt;- data$time # Convert time to seconds coords$time &lt;- as.numeric(difftime(coords$time, min(coords$time), units = &quot;secs&quot;)) coords &lt;- coords %&gt;% rename(Longitude = X, Latitude = Y, Elevation = ele, Time = time) # Data for plotting coords_plot &lt;- coords %&gt;% pivot_longer(-Time, names_to = &quot;Variable&quot;, values_to = &quot;Value&quot;) ggplot(coords_plot, aes(x = Time, y = Value, color = Variable)) + geom_point(show.legend = F) + facet_wrap(~Variable, scales = &quot;free_y&quot;) + scale_color_brewer(palette = &quot;Set1&quot;) + theme_bw() + theme(text = element_text(size = 18)) 7.5 Fit a statistical or machine learning model to your movement data. Obtain predictions of your location on a fine time scale so that the estimates resemble a continuous trajectory. # Fit model to longitude using time as a predictor m1_long &lt;- lm(Longitude ~ poly(Time, degree = 20, raw = T), data = coords) m2_long &lt;- gam(Longitude ~ s(Time, bs=&quot;gp&quot;, k=50), data = coords) #summary(m1_long) #summary(m2_long) # Fit model to latitude using time as a predictor m1_lat &lt;- lm(Latitude ~ poly(Time, degree = 20, raw = T), data = coords) m2_lat &lt;- gam(Latitude ~ s(Time, bs=&quot;gp&quot;, k=50), data = coords) #summary(m1_lat) #summary(m2_lat) # Fit model to elevation using time as a predictor m1_elev &lt;- lm(Elevation ~ poly(Time, degree = 20, raw = T), data = coords) m2_elev &lt;- gam(Elevation ~ s(Time, bs=&quot;gp&quot;, k=50), data = coords) #summary(m1_elev) #summary(m2_elev) # Estimate movement trajectory at a very fine temporal scale (every 1/2th of a second) df.pred &lt;- data.frame(Time = seq(0, as.numeric(max(coords$Time)), 0.5)) df.pred$lon.m1.hat = predict(m1_long, newdata = df.pred) df.pred$lon.m2.hat = predict(m2_long, newdata = df.pred) df.pred$lat.m1.hat = predict(m1_lat, newdata = df.pred) df.pred$lat.m2.hat = predict(m2_lat, newdata = df.pred) df.pred$elev.m1.hat = predict(m1_elev, newdata = df.pred) df.pred$elev.m2.hat = predict(m2_elev, newdata = df.pred) 7.6 Plot/map your estimated trajectory from 5. Explore your estimated trajectory as best as possible using the plots/maps. Note any unique features or shortcomings of your model. # Plot for Longitude plt.long &lt;- ggplot() + geom_point(data = coords, aes(x = Time, y = Longitude), size = 2, color = &quot;grey&quot;) + geom_line(data = df.pred, aes(x = Time, y = lon.m1.hat), color = &quot;navy&quot;, size = 1) + geom_line(data = df.pred, aes(x = Time, y = lon.m2.hat), color = &quot;darkorange&quot;, size = 1) + labs(x = &quot;Time&quot;, y = &quot;Longitude&quot;) + theme_bw() + theme(text = element_text(size = 18)) # Plot for Latitude plt.lat &lt;- ggplot() + geom_point(data = coords, aes(x = Time, y = Latitude, color = &quot;Observed data&quot;), size = 2) + geom_line(data = df.pred, aes(x = Time, y = lat.m1.hat, color = &quot;Polynomial prediction&quot;), size = 1) + geom_line(data = df.pred, aes(x = Time, y = lat.m2.hat, color = &quot;GAM prediction&quot;), size = 1) + scale_color_manual(values = c(&quot;Polynomial prediction&quot; = &quot;navy&quot;, &quot;GAM prediction&quot; = &quot;darkorange&quot;, &quot;Observed data&quot; = &quot;grey50&quot;)) + labs(x = &quot;Time&quot;, y = &quot;Latitude&quot;) + theme_bw() + theme(text = element_text(size = 18), legend.position = &quot;top&quot;, legend.direction = &quot;vertical&quot;, legend.title = element_blank()) # Plot for Elevation plt.elev &lt;- ggplot() + geom_point(data = coords, aes(x = Time, y = Elevation), size = 2, color = &quot;grey&quot;) + geom_line(data = df.pred, aes(x = Time, y = elev.m1.hat), color = &quot;navy&quot;, size = 1) + geom_line(data = df.pred, aes(x = Time, y = elev.m2.hat), color = &quot;darkorange&quot;, size = 1) + labs(x = &quot;Time&quot;, y = &quot;Elevation&quot;) + theme_bw() + theme(text = element_text(size = 18)) plt.long + plt.lat + plt.elev # Create data frame for plotting df.pred.poly &lt;- df.pred %&gt;% dplyr::select(Time, lon.m1.hat, lat.m1.hat) %&gt;% rename(Longitude = lon.m1.hat, Latitude = lat.m1.hat) %&gt;% mutate(Model = &quot;Polynomial&quot;) df.pred.cart &lt;- df.pred %&gt;% dplyr::select(Time, lon.m2.hat, lat.m2.hat) %&gt;% rename(Longitude = lon.m2.hat, Latitude = lat.m2.hat) %&gt;% mutate(Model = &quot;GAM&quot;) df.pred.map &lt;- df.pred.cart %&gt;% rbind(df.pred.poly) %&gt;% rbind(coords %&gt;% dplyr::select(-Elevation) %&gt;% mutate(Model = &quot;Observed&quot;)) color.model &lt;- colorFactor(palette = c(&quot;Polynomial&quot; = &quot;darkorange&quot;, &quot;GAM&quot; = &quot;navy&quot;, &quot;Observed&quot; = &quot;yellow&quot;), domain = df.pred.map$Model) # Plot the movement data leaflet(df.pred.map) %&gt;% addProviderTiles(providers$Esri.WorldImagery) %&gt;% addCircleMarkers( ~Longitude, ~Latitude, fillColor = ~color.model(Model), color = ~&quot;grey50&quot;, radius = 3, weight = 0.5, fillOpacity = 1) %&gt;% addLegend( title = &quot;Model&quot;, position = &quot;topright&quot;, pal = color.model, values = ~Model, opacity = 1) 7.7 Estimate a feature or quantity of interest from your estimated trajectory (e.g., velocity, residence time, number of contacts, etc) # Calculate speed observed data dist &lt;- st_distance(data$geometry[1:809], data$geometry[2:810], by_element = T) (sum(dist, na.rm = T)/1000)*.62 # looks okay (Strava = 0.74 mi) ## 0.7566649 [m] speed &lt;- (dist/as.numeric(diff(data$time)))*2.24 plot(coords$Time[-1], speed, xlab=&quot;Time (seconds)&quot;,ylab=&quot;Velocity (miles per hour)&quot;, main = &#39;Observed&#39;) #Convert model coordinates to sf object data.hat.m1 &lt;- st_as_sf(df.pred, coords = c(&quot;lon.m1.hat&quot;, &quot;lat.m1.hat&quot;), crs = st_crs(data)) data.hat.m2 &lt;- st_as_sf(df.pred, coords = c(&quot;lon.m2.hat&quot;, &quot;lat.m2.hat&quot;), crs = st_crs(data)) # Calculate speed Polynomial dist.hat.m1 &lt;- st_distance(data.hat.m1$geometry[1:1618], data.hat.m1$geometry[2:1619], by_element = T) (sum(dist.hat.m1)/1000)*.62 ## 0.7260518 [m] speed.hat.m1 &lt;- (dist.hat.m1/as.numeric(diff(data.hat.m1$Time)))*2.24 plot(data.hat.m1$Time[-1], speed.hat.m1, xlab=&quot;Time (seconds)&quot;,ylab=&quot;Velocity (miles per hour)&quot;, main = &#39;Model: Polynomial&#39;) # Calculate speed GAM dist.hat.m2 &lt;- st_distance(data.hat.m2$geometry[1:1618], data.hat.m2$geometry[2:1619], by_element = T) (sum(dist.hat.m2)/1000)*.62 ## 0.7277217 [m] speed.hat.m1 &lt;- (dist.hat.m2/as.numeric(diff(data.hat.m2$Time)))*2.24 plot(data.hat.m2$Time[-1], speed.hat.m1, xlab=&quot;Time (seconds)&quot;,ylab=&quot;Velocity (miles per hour)&quot;, main = &#39;Model: GAM&#39;) "],["activity-2.html", "Chapter 8 Activity 2 8.1 Load packages 8.2 Chose an area on or close to campus where it is easy for you to understand how the elevation changes. For example, I chose the parking lot outside of Dickens Hall. Using a smartphone record the elevation at several locations (points) within the area you chose. I recommend using the app Strava, but you can use whatever you want. 8.3 Obtain a .gpx or .csv file for your elevation data. At minimum the file should contain the location and time of the elevation measurements. 8.4 Plot/map your elevation data. I would recommend using R and/or Google earth. 8.5 Explore your elevation data. For example, are there any unique features of your data? Do your data contain obvious measurement error (e.g., an elevation that can’t possibly be true)? Really try to explore your data as best as possible using the plots/maps you made in. 8.6 Write out the goals that you wish to accomplish using your elevation data. For example, my goal was to make a map of the Dicken’s Hall parking lot. This involves using the elevation data I collected to make predictions of the elevation at any possible spatial locations within the parking lot. I would also like to make inference about the location where the elevation is lowest within the parking lot. 8.7 Write out several statistical or machine learning models that you think you can use to answer the questions/goals you wrote in prompt #5. Be as creative and inclusive here. For each statistical or machine learning model, make sure you explain each component (piece) of the model 8.8 Of the models you developed in prompt #6, find (or develop) software to fit at least two of these models to your elevation data. Note that in a perfect world, you would be able to either find existing software or develop new programs that enable you to fit any statistical or machine learning model you want. In reality, you may may end up having to make some unwanted changes to your models in prompt #6 to be able to find existing software to fit these models to the data. 8.9 Related to prompt #5, use both models you fit to your elevation data in prompt #7 to answer the questions/goals. For my elevation data, this would include making a predictive heatmap showing the elevation of the Dickens Hall parking lot and then estimating the coordinates of the point where elevation is at a minimum. 8.10 Based on the material in Chapter 6 of Spatio-Temporal Statistics with R and our discussion in class on March 26, compare, check and evaluate the two models from #8. 8.11 Check model performance", " Chapter 8 Activity 2 The goal of this activity is to visualize the elevation data of an experimental site in the North Agronomy Farm at Kansas State University. Also my goal is to predict elevation with different models and infer at what location we observe the lowest and highest elevation point. 8.1 Load packages library(sf) library(sp) library(raster) library(tidyverse) 8.2 Chose an area on or close to campus where it is easy for you to understand how the elevation changes. For example, I chose the parking lot outside of Dickens Hall. Using a smartphone record the elevation at several locations (points) within the area you chose. I recommend using the app Strava, but you can use whatever you want. The data was collected near where I live. 8.3 Obtain a .gpx or .csv file for your elevation data. At minimum the file should contain the location and time of the elevation measurements. Two .gpx files were generated: one for the area boundaries and one for the elevation data. 8.4 Plot/map your elevation data. I would recommend using R and/or Google earth. download.file(&quot;http://www2.census.gov/geo/tiger/GENZ2015/shp/cb_2015_us_state_20m.zip&quot;, destfile = &quot;states.zip&quot;) unzip(&quot;states.zip&quot;) sf.us &lt;- st_read(&quot;cb_2015_us_state_20m.shp&quot;) ## Reading layer `cb_2015_us_state_20m&#39; from data source ## `C:\\Users\\luizfelipeaa\\OneDrive - Kansas State University\\Documents\\STAT764\\stat764-portfolio\\cb_2015_us_state_20m.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 52 features and 9 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -179.1743 ymin: 17.91377 xmax: 179.7739 ymax: 71.35256 ## Geodetic CRS: NAD83 sf.kansas &lt;- sf.us[48,6] sf.kansas &lt;- as(sf.kansas, &#39;Spatial&#39;) url1 &lt;- &quot;https://www.dropbox.com/scl/fi/cfvdqoe9y7nx2yebvlusf/hill2.gpx?rlkey=orq8idvojvgxyo0orwtg0w5g4&amp;dl=1&quot; pt.study.area &lt;- st_read(dsn=url1,layer=&quot;track_points&quot;) ## Reading layer `track_points&#39; from data source `https://www.dropbox.com/scl/fi/cfvdqoe9y7nx2yebvlusf/hill2.gpx?rlkey=orq8idvojvgxyo0orwtg0w5g4&amp;dl=1&#39; using driver `GPX&#39; ## Simple feature collection with 99 features and 26 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -96.64579 ymin: 39.21366 xmax: -96.64525 ymax: 39.21396 ## Geodetic CRS: WGS 84 sf.study.area &lt;- st_polygon(list(rbind(st_coordinates(pt.study.area),st_coordinates(pt.study.area)[1,]))) sf.study.area &lt;- st_buffer(sf.study.area, .00006) sf.study.area &lt;- st_sf(st_sfc(sf.study.area), crs = crs(sf.kansas)) # Plot study area plot(sf.study.area, col=&quot;lightgreen&quot;) mapview::mapview(sf.study.area) # Obtain elevation data url2 &lt;- &quot;https://www.dropbox.com/scl/fi/ylx9yc62ajeba5wd7iuum/hill1.gpx?rlkey=ah3qh1ogtq6z41wxs2c303e2a&amp;dl=1&quot; pt.elev &lt;- st_read(dsn=url2,layer=&quot;track_points&quot;) ## Reading layer `track_points&#39; from data source `https://www.dropbox.com/scl/fi/ylx9yc62ajeba5wd7iuum/hill1.gpx?rlkey=ah3qh1ogtq6z41wxs2c303e2a&amp;dl=1&#39; using driver `GPX&#39; ## Simple feature collection with 92 features and 26 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -96.64572 ymin: 39.21369 xmax: -96.6453 ymax: 39.21395 ## Geodetic CRS: WGS 84 pt.elev &lt;- pt.elev[,4] # Keep only elevation pt.elev &lt;- pt.elev[-c(1:15),] pt.elev &lt;- rbind(pt.elev,pt.study.area[,4]) 8.5 Explore your elevation data. For example, are there any unique features of your data? Do your data contain obvious measurement error (e.g., an elevation that can’t possibly be true)? Really try to explore your data as best as possible using the plots/maps you made in. From my perspective of the area where I collected the data, there appears to be measurement error in the data points. It seems that the GPS was not able to accurately distinguish some of the locations, possibly because I was collecting the data at varying speeds. plot(sf.study.area) plot(pt.elev,add=TRUE) hist(pt.elev$ele,col=&quot;grey&quot;) summary(pt.elev$ele) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 389.3 389.7 389.9 389.8 390.0 390.1 ggplot() + geom_sf(data=sf.study.area, color = &quot;black&quot;) + geom_sf(data=pt.elev, aes(color = ele), size = 4)+ scale_color_gradient(low=&quot;red&quot;, high=&quot;darkgreen&quot;, name = &quot;Elevation (m)&quot;)+ theme_bw() # Transform to a planar coordinate reference system pt.elev.utm &lt;- st_transform(pt.elev,CRS(&quot;+proj=utm +zone=14 +datum=WGS84 +units=m&quot;)) sf.study.area.utm &lt;- st_transform(sf.study.area,CRS(&quot;+proj=utm +zone=14 +datum=WGS84 +units=m&quot;)) # Dataframe for the models df.elev &lt;- data.frame (elev = pt.elev$ele, long = st_coordinates(pt.elev)[,1], lat = st_coordinates(pt.elev)[,2], s1 = st_coordinates(pt.elev.utm)[,1], s2 = st_coordinates(pt.elev.utm)[,2]) 8.6 Write out the goals that you wish to accomplish using your elevation data. For example, my goal was to make a map of the Dicken’s Hall parking lot. This involves using the elevation data I collected to make predictions of the elevation at any possible spatial locations within the parking lot. I would also like to make inference about the location where the elevation is lowest within the parking lot. My goal is to obtain the most precise predictions of the area’s elevation and determine the location with the lowest elevation. 8.7 Write out several statistical or machine learning models that you think you can use to answer the questions/goals you wrote in prompt #5. Be as creative and inclusive here. For each statistical or machine learning model, make sure you explain each component (piece) of the model Linear model: \\(y_i = \\beta_0 + \\beta_1 \\cdot Long + \\beta_2 \\cdot Long^2 + \\beta_3 \\cdot Lat + \\beta_4 \\cdot Lat^2 + \\epsilon_i\\) \\(\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\\) GAM Model: \\(y_i= \\beta_0 + f(Long, Lat) + \\epsilon_i\\) \\(\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\\) 8.8 Of the models you developed in prompt #6, find (or develop) software to fit at least two of these models to your elevation data. Note that in a perfect world, you would be able to either find existing software or develop new programs that enable you to fit any statistical or machine learning model you want. In reality, you may may end up having to make some unwanted changes to your models in prompt #6 to be able to find existing software to fit these models to the data. Linear model # Statistical analysis 1: non-hierarchical linear model with iid errors m1 &lt;- lm(elev~s1+I(s1^2)+s2+I(s2^2),data=df.elev) # Make raster of study area to be able to map predictions from m1 rl.E.y_lin &lt;- raster(,nrow=100,ncols=100,ext=extent(sf.study.area.utm),crs=crs(sf.study.area.utm)) # Make data.frame to be able to make predictions at each pixel (cell of raster) df.pred &lt;- data.frame(elev = NA, s1 = xyFromCell(rl.E.y_lin,cell=1:length(rl.E.y_lin[]))[,1], s2 = xyFromCell(rl.E.y_lin,cell=1:length(rl.E.y_lin[]))[,2]) # Make spatial predictions at each pixel df.pred$elev &lt;- predict(m1,df.pred[,2:3]) ## Warning in predict.lm(m1, df.pred[, 2:3]): prediction from a rank-deficient fit may be misleading # View first 6 rows of predictions head(df.pred) ## elev s1 s2 ## 1 389.8696 703247.1 4343168 ## 2 389.8708 703247.7 4343168 ## 3 389.8720 703248.2 4343168 ## 4 389.8732 703248.8 4343168 ## 5 389.8744 703249.4 4343168 ## 6 389.8757 703250.0 4343168 # Fill raster file with predictions rl.E.y_lin[] &lt;- c(df.pred$elev) rl.E.y_lin &lt;- mask(rl.E.y_lin,sf.study.area.utm) # Estimate coordinates and amount of maximum elevation xyFromCell(rl.E.y_lin,cell=which.max(rl.E.y_lin[])) ## x y ## [1,] 703297.8 4343162 rl.E.y_lin[which.max(rl.E.y_lin[])] ## [1] 389.9558 rl.E.y_lin_df &lt;- as.data.frame(raster::rasterToPoints(rl.E.y_lin)) colnames(rl.E.y_lin_df) &lt;- c(&quot;lon&quot;, &quot;lat&quot;, &quot;elevation&quot;) min_elevation_value_lin &lt;- min(rl.E.y_lin_df$elevation) min_elevation_point_df_lin &lt;- rl.E.y_lin_df[which.min(rl.E.y_lin_df$elevation), ] ggplot() + geom_raster(data = rl.E.y_lin_df, aes(x = lon, y = lat, fill = elevation)) + geom_sf(data = sf.study.area.utm, fill = NA, color = &quot;black&quot;) + geom_point(data = min_elevation_point_df_lin, aes(x = lon, y = lat, color = &quot;Mininum elevation&quot;), size = 8, shape = 8) + labs(title = &quot;Linear Model&quot;, x = &quot;Longitude&quot;, y = &quot;Latitude&quot;, color = &quot;&quot;) + scale_fill_viridis_c(name = &quot;Elevation (m)&quot;) + scale_color_manual(values = c(&quot;Mininum elevation&quot; = &quot;red&quot;)) + theme_bw() GAM model # Try low-rank Gaussian process (i.e., modern kriging model) library(mgcv) m2 &lt;- gam(elev~s(s1,s2,bs=&quot;gp&quot;,k=50),data=df.elev) # Make raster of study area to be able to map predictions from m2 rl.E.y_gam &lt;- raster(,nrow=100,ncols=100,ext=extent(sf.study.area.utm),crs=crs(sf.study.area.utm)) # Make data.frame to be able to make predictions at each pixel (cell of raster) df.pred &lt;- data.frame(elev = NA, s1 = xyFromCell(rl.E.y_gam,cell=1:length(rl.E.y_gam[]))[,1], s2 = xyFromCell(rl.E.y_gam,cell=1:length(rl.E.y_gam[]))[,2]) # Make spatial predictions at each pixel df.pred$elev &lt;- predict(m2,df.pred[,2:3]) # View first 6 rows of predictions head(df.pred) ## elev s1 s2 ## 1 390.4175 703247.1 4343168 ## 2 390.4056 703247.7 4343168 ## 3 390.3936 703248.2 4343168 ## 4 390.3814 703248.8 4343168 ## 5 390.3692 703249.4 4343168 ## 6 390.3570 703250.0 4343168 # Fill raster file with predictions rl.E.y_gam[] &lt;- c(df.pred$elev) rl.E.y_gam &lt;- mask(rl.E.y_gam,sf.study.area.utm) # Estimate coordinates and amount of maximum elevation xyFromCell(rl.E.y_gam,cell=which.max(rl.E.y_gam[])) ## x y ## [1,] 703297.8 4343162 rl.E.y_gam[which.max(rl.E.y_gam[])] ## [1] 390.5805 rl.E.y_gam_df &lt;- as.data.frame(raster::rasterToPoints(rl.E.y_gam)) colnames(rl.E.y_gam_df) &lt;- c(&quot;lon&quot;, &quot;lat&quot;, &quot;elevation&quot;) min_elevation_value &lt;- min(rl.E.y_gam_df$elevation) min_elevation_point_df &lt;- rl.E.y_gam_df[which.min(rl.E.y_gam_df$elevation), ] ggplot() + geom_raster(data = rl.E.y_gam_df, aes(x = lon, y = lat, fill = elevation)) + geom_sf(data = sf.study.area.utm, fill = NA, color = &quot;black&quot;) + geom_point(data = min_elevation_point_df, aes(x = lon, y = lat, color = &quot;Mininum elevation&quot;), size = 8, shape = 8) + labs(title = &quot;GAM Model&quot;, x = &quot;Longitude&quot;, y = &quot;Latitude&quot;, color = &quot;&quot;) + scale_fill_viridis_c(name = &quot;Elevation (m)&quot;) + scale_color_manual(values = c(&quot;Mininum elevation&quot; = &quot;red&quot;)) + theme_bw() 8.9 Related to prompt #5, use both models you fit to your elevation data in prompt #7 to answer the questions/goals. For my elevation data, this would include making a predictive heatmap showing the elevation of the Dickens Hall parking lot and then estimating the coordinates of the point where elevation is at a minimum. The GAM model does a way better job on predicting the lowest elevation point (validated by myself). 8.10 Based on the material in Chapter 6 of Spatio-Temporal Statistics with R and our discussion in class on March 26, compare, check and evaluate the two models from #8. set.seed(123) trainIndices &lt;- sample(1:nrow(df.elev), size = floor(0.7 * nrow(df.elev))) # Subset the data into train and test sets train.elev &lt;- df.elev[trainIndices, ] test.elev &lt;- df.elev[-trainIndices, ] Linear model (Train set) # Statistical analysis 1: non-hierarchical linear model with iid errors m1 &lt;- lm(elev~s1+I(s1^2)+s2+I(s2^2),data=train.elev) # Make raster of study area to be able to map predictions from m1 rl.E.y_lin &lt;- raster(,nrow=100,ncols=100,ext=extent(sf.study.area.utm),crs=crs(sf.study.area.utm)) # Make data.frame to be able to make predictions at each pixel (cell of raster) df.pred &lt;- data.frame(elev = NA, s1 = xyFromCell(rl.E.y_lin,cell=1:length(rl.E.y_lin[]))[,1], s2 = xyFromCell(rl.E.y_lin,cell=1:length(rl.E.y_lin[]))[,2]) # Make spatial predictions at each pixel df.pred$elev &lt;- predict(m1,df.pred[,2:3]) ## Warning in predict.lm(m1, df.pred[, 2:3]): prediction from a rank-deficient fit may be misleading # View first 6 rows of predictions head(df.pred) ## elev s1 s2 ## 1 389.9443 703247.1 4343168 ## 2 389.9453 703247.7 4343168 ## 3 389.9464 703248.2 4343168 ## 4 389.9475 703248.8 4343168 ## 5 389.9486 703249.4 4343168 ## 6 389.9497 703250.0 4343168 # Fill raster file with predictions rl.E.y_lin[] &lt;- c(df.pred$elev) rl.E.y_lin &lt;- mask(rl.E.y_lin,sf.study.area.utm) # Estimate coordinates and amount of maximum elevation xyFromCell(rl.E.y_lin,cell=which.max(rl.E.y_lin[])) ## x y ## [1,] 703293.2 4343164 rl.E.y_lin[which.max(rl.E.y_lin[])] ## [1] 390.0053 rl.E.y_lin_df &lt;- as.data.frame(raster::rasterToPoints(rl.E.y_lin)) colnames(rl.E.y_lin_df) &lt;- c(&quot;lon&quot;, &quot;lat&quot;, &quot;elevation&quot;) min_elevation_value_lin &lt;- min(rl.E.y_lin_df$elevation) min_elevation_point_df_lin &lt;- rl.E.y_lin_df[which.min(rl.E.y_lin_df$elevation), ] ggplot() + geom_raster(data = rl.E.y_lin_df, aes(x = lon, y = lat, fill = elevation)) + geom_sf(data = sf.study.area.utm, fill = NA, color = &quot;black&quot;) + geom_point(data = min_elevation_point_df_lin, aes(x = lon, y = lat, color = &quot;Mininum elevation&quot;), size = 8, shape = 8) + labs(title = &quot;Linear Model&quot;, x = &quot;Longitude&quot;, y = &quot;Latitude&quot;, color = &quot;&quot;) + scale_fill_viridis_c(name = &quot;Elevation (m)&quot;) + scale_color_manual(values = c(&quot;Mininum elevation&quot; = &quot;red&quot;)) + theme_bw() GAM model (Train set) # Try low-rank Gaussian process (i.e., modern kriging model) library(mgcv) m2 &lt;- gam(elev~s(s1,s2,bs=&quot;gp&quot;,k=50),data=train.elev) # Make raster of study area to be able to map predictions from m2 rl.E.y_gam &lt;- raster(,nrow=100,ncols=100,ext=extent(sf.study.area.utm),crs=crs(sf.study.area.utm)) # Make data.frame to be able to make predictions at each pixel (cell of raster) df.pred &lt;- data.frame(elev = NA, s1 = xyFromCell(rl.E.y_gam,cell=1:length(rl.E.y_gam[]))[,1], s2 = xyFromCell(rl.E.y_gam,cell=1:length(rl.E.y_gam[]))[,2]) # Make spatial predictions at each pixel df.pred$elev &lt;- predict(m2,df.pred[,2:3]) # View first 6 rows of predictions head(df.pred) ## elev s1 s2 ## 1 390.1689 703247.1 4343168 ## 2 390.1676 703247.7 4343168 ## 3 390.1663 703248.2 4343168 ## 4 390.1650 703248.8 4343168 ## 5 390.1637 703249.4 4343168 ## 6 390.1626 703250.0 4343168 # Fill raster file with predictions rl.E.y_gam[] &lt;- c(df.pred$elev) rl.E.y_gam &lt;- mask(rl.E.y_gam,sf.study.area.utm) # Estimate coordinates and amount of maximum elevation xyFromCell(rl.E.y_gam,cell=which.max(rl.E.y_gam[])) ## x y ## [1,] 703293.2 4343125 rl.E.y_gam[which.max(rl.E.y_gam[])] ## [1] 391.2241 rl.E.y_gam_df &lt;- as.data.frame(raster::rasterToPoints(rl.E.y_gam)) colnames(rl.E.y_gam_df) &lt;- c(&quot;lon&quot;, &quot;lat&quot;, &quot;elevation&quot;) min_elevation_value &lt;- min(rl.E.y_gam_df$elevation) min_elevation_point_df &lt;- rl.E.y_gam_df[which.min(rl.E.y_gam_df$elevation), ] ggplot() + geom_raster(data = rl.E.y_gam_df, aes(x = lon, y = lat, fill = elevation)) + geom_sf(data = sf.study.area.utm, fill = NA, color = &quot;black&quot;) + geom_point(data = min_elevation_point_df, aes(x = lon, y = lat, color = &quot;Mininum elevation&quot;), size = 8, shape = 8) + labs(title = &quot;GAM Model&quot;, x = &quot;Longitude&quot;, y = &quot;Latitude&quot;, color = &quot;&quot;) + scale_fill_viridis_c(name = &quot;Elevation (m)&quot;) + scale_color_manual(values = c(&quot;Mininum elevation&quot; = &quot;red&quot;)) + theme_bw() 8.11 Check model performance Linear model (Test set) # Compare point prediction of the the expected value of elevation (E(y)) to # observed records from new data set. E.y.Linear &lt;- predict(m1,newdata=test.elev) ## Warning in predict.lm(m1, newdata = test.elev): prediction from a rank-deficient fit may be misleading plot(E.y.Linear,test.elev$elev,xlab=&quot;Predicted expected value&quot;,ylab=&quot;New observed elevation&quot;) # Quantify predictive accuracy using scoring rule # For more details see Ch.6 or https://www.annualreviews.org/content/journals/10.1146/annurev-statistics-062713-085831 sum(dnorm(test.elev$elev,E.y.Linear,log=TRUE)) # logarithmic scoring rule (proper scoring rule for linear model) ## [1] -50.14819 mean((test.elev$elev - E.y.Linear)^2) # Mean square error. Commonly used scoring rule, but not proper scoring rule for ALL situations ## [1] 0.05450753 mean(abs(test.elev$elev - E.y.Linear)) # Mean absolute error. Commonly used scoring rule, but not proper scoring rule for ALL situations ## [1] 0.2007308 # Quantify calibration of predictive intervals. Note that the code below only works # with the linear model (i.e., lm function). It is somewhat difficult or impossible to # get prediction intervals from the other models (e.g., difficult = gam; impossible = regression tree) PI.Linear &lt;- predict(m1,newdata=test.elev, interval = c(&quot;prediction&quot;), level = 0.95) ## Warning in predict.lm(m1, newdata = test.elev, interval = c(&quot;prediction&quot;), : prediction from a rank-deficient fit may be misleading # Determine what % of the new observations fall within the # prediction intervals. This % is called calibration. A 95% # prediction interval should cover 95% of new observations # if it is calibrated and the model doesn&#39;t have issues that # need to be fixed. mean(ifelse(test.elev$elev&gt;PI.Linear[,2],1,0)*ifelse(test.elev$elev&lt;PI.Linear[,3],1,0)) ## [1] 0.9433962 GAM model (Test set) # Compare point prediction of the the expected value of elevation (E(y)) to # observed records from new data set. E.y.GAM &lt;- predict(m2,newdata=test.elev) plot(E.y.GAM,test.elev$elev,xlab=&quot;Predicted expected value&quot;,ylab=&quot;New observed elevation&quot;) # Quantify predictive accuracy using scoring rule # For more details see Ch.6 or https://www.annualreviews.org/content/journals/10.1146/annurev-statistics-062713-085831 sum(dnorm(test.elev$elev,E.y.GAM,log=TRUE)) # logarithmic scoring rule (proper scoring rule for linear model) ## [1] -49.33706 mean((test.elev$elev - E.y.GAM)^2) # Mean square error. Commonly used scoring rule, but not proper scoring rule for ALL situations ## [1] 0.02389873 mean(abs(test.elev$elev - E.y.GAM)) # Mean absolute error. Commonly used scoring rule, but not proper scoring rule for ALL situations ## [1] 0.1105208 Overall, the GAM model resulted superior compared to the linear model. For all the scoring rules, the GAM model showed a higher predictive accuracy. "],["activity-3.html", "Chapter 9 Activity 3 9.1 Load packages 9.2 Import KS map and aphid data 9.3 Data visualization 9.4 For the data on the abundance of English grain aphids, propose three different statistical models (or machine learning approach) that are capable of predicting the number of English grain aphids at any location within the state of Kansas at any time for the years 2014 and 2015. Make sure to write out the three statistical models using formal notation and fully describe each component using words. 9.5 For the three statistical models you proposed in question #1, propose a way to measure the accuracy (and perhaps the calibration) of predictions. 9.6 Fit the three statistical models you proposed in question #1 to the English grain aphid abundance data. 9.7 For the three models you fit in question #3, which model makes the most accurate predictions? How good is the best model in real world terms? Remember we are trying to predict the number of English grain aphids, which is a count! 9.8 Summarize your results using words, numerical values and figures/maps.", " Chapter 9 Activity 3 9.1 Load packages library(sf) library(sp) library(raster) library(mgcv) library(ggplot2) library(dplyr) 9.2 Import KS map and aphid data # Download data on English grain aphid #url &lt;- &quot;https://www.dropbox.com/scl/fi/9ymxt900s77uq50ca6dgc/Enders-et-al.-2018-data.csv?rlkey=0rxjwleenhgu0gvzow5p0x9xf&amp;dl=1&quot; #data &lt;- read.csv(url) #data &lt;- data[,c(2,8:10)] # Keep only the data on English grain aphid # saveRDS(data, &quot;A3_data.RDS&quot;) data &lt;- readRDS(&quot;A3_data.RDS&quot;) pts.sample &lt;- data coordinates(pts.sample) =~ long + lat proj4string(pts.sample) &lt;- CRS(&quot;+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0&quot;) 9.3 Data visualization # Download KS shapefile #ks &lt;- raster::getData(name=&quot;GADM&quot;, country=&quot;USA&quot;, level=1) %&gt;% # st_as_sf() %&gt;% # dplyr::filter(NAME_1 == &quot;Kansas&quot;) #saveRDS(ks, &quot;A3_shpKS.RDS&quot;) ks &lt;- readRDS(&quot;A3_shpKS.RDS&quot;) coordinates &lt;- st_as_sf(data, coords = c(&quot;long&quot;, &quot;lat&quot;), crs = st_crs(ks)) # Plot ggplot() + geom_sf(data = ks, fill = &quot;grey95&quot;, color = &quot;black&quot;) + geom_point(data = data, aes(x = long, y = lat, size = EGA), color = &quot;red&quot;, shape = 21) + labs(title = &quot;Abundance of EGA&quot;, x = &quot;Longitude&quot;, y = &quot;Latitude&quot;) + facet_wrap(~ year) + theme_bw() hist(data$EGA) summary(data$EGA) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.00 0.00 2.00 28.33 15.00 940.00 # Download National Land Cover Database # url.nlcd &lt;- &quot;https://www.dropbox.com/scl/fi/ew7yzm93aes7l8l37cn65/KS_2011_NLCD.img?rlkey=60ahyvxhq18gt0yr47tuq5fig&amp;dl=1&quot; # rl.nlcd2011 &lt;- raster(url.nlcd) #saveRDS(rl.nlcd2011, &quot;A3_nlcd2011.RDS&quot;) rl.nlcd2011 &lt;- readRDS(&quot;A3_nlcd2011.RDS&quot;) # Make raster file that contains pixels with value of 1 if grassland and # zero if other type of land cover. plot(rl.nlcd2011) rl.nlcd.grass &lt;- rl.nlcd2011 rl.nlcd.grass[] &lt;- ifelse(rl.nlcd.grass[]==71,1,0) plot(rl.nlcd.grass) # Calculate percentage of land area that is grassland withing 5 km of sampled location data$grass.perc &lt;- unlist(lapply(extract(rl.nlcd.grass,pts.sample,buffer=5000),mean))*100 hist(data$grass.perc,col=&quot;grey&quot;,main=&quot;&quot;,xlab=&quot;% grassland within \\n5 km at sample location&quot;) 9.4 For the data on the abundance of English grain aphids, propose three different statistical models (or machine learning approach) that are capable of predicting the number of English grain aphids at any location within the state of Kansas at any time for the years 2014 and 2015. Make sure to write out the three statistical models using formal notation and fully describe each component using words. Model 1 \\[Z = y\\] \\[[y|\\lambda] = Poisson(\\lambda) \\] \\[\\eta_s \\sim MVN(0, \\Sigma)\\] \\[E(y) = e^{\\beta_0+ \\beta_1 \\cdot X \\eta_s+\\eta_t}\\] Model 2 \\[Z = y\\] \\[[y|r,p] = NB(r, p)\\] \\[\\eta_s \\sim MVN(0, \\Sigma)\\] \\[E(y) = e^{\\beta_0+ \\beta_1 \\cdot X \\eta_s+\\eta_t}\\] Model 3 \\[Z = y\\] \\[[y|p, \\lambda] = ZIP(p,\\lambda)\\] \\[\\eta_s \\sim MVN(0, \\Sigma)\\] \\[E(y) = e^{\\beta_0+\\beta_1 \\cdot X +\\eta_s+\\eta_t}\\] 9.5 For the three statistical models you proposed in question #1, propose a way to measure the accuracy (and perhaps the calibration) of predictions. To measure the accuracy of the models we can calculate the Akaike information criteria (AIC), the mean absolute (MEA) and mean square error (MSE). \\(AIC=2k-2 \\mathrm {\\ln}(\\hat {L})\\) \\(k\\) = number of estimated parameters in the model \\(\\hat {L}\\) = maximum value of the likelihood function for the model \\(MAE = \\frac{\\sum^n_{i=1} |y_i - x_i|}{n}\\) \\(y_i\\) = prediction \\(x_i\\) = true value \\(n\\) = total number of data points \\(MSE = \\frac{1}{n} \\sum^n_{i=1}{(Y_i - \\hat{Y}_i)^2}\\) \\({n}\\) = number of data points \\(Y_{i}\\) = observed values \\(\\hat{Y}_{i}\\) = predicted values 9.6 Fit the three statistical models you proposed in question #1 to the English grain aphid abundance data. set.seed(100) df.sample &lt;- sample(c(TRUE,FALSE), nrow(data), replace=TRUE, prob=c(0.5,0.5)) df.train &lt;- data[df.sample,] df.test &lt;- data[!df.sample,] m1 &lt;- gam(EGA ~ grass.perc + as.factor(year) + s(long,lat, bs = &quot;gp&quot;), family = poisson(link = &quot;log&quot;), data = df.train) summary(m1) ## ## Family: poisson ## Link function: log ## ## Formula: ## EGA ~ grass.perc + as.factor(year) + s(long, lat, bs = &quot;gp&quot;) ## ## Parametric coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -4.072721 0.508579 -8.008 1.17e-15 *** ## grass.perc -0.034166 0.001664 -20.538 &lt; 2e-16 *** ## as.factor(year)2015 6.148542 0.500663 12.281 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df Chi.sq p-value ## s(long,lat) 31.45 31.8 5663 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.67 Deviance explained = 81.1% ## UBRE = 17.558 Scale est. = 1 n = 178 m2 &lt;- gam(EGA ~ grass.perc + as.factor(year) + s(long,lat, bs = &quot;gp&quot;), family = nb(theta = NULL, link = &quot;log&quot;), data = df.train) summary(m2) ## ## Family: Negative Binomial(0.653) ## Link function: log ## ## Formula: ## EGA ~ grass.perc + as.factor(year) + s(long, lat, bs = &quot;gp&quot;) ## ## Parametric coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.29390 0.61890 -5.322 1.03e-07 *** ## grass.perc -0.00141 0.00678 -0.208 0.835 ## as.factor(year)2015 5.67995 0.59656 9.521 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df Chi.sq p-value ## s(long,lat) 8.356 11.02 224.3 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.263 Deviance explained = 72.7% ## -REML = 493.04 Scale est. = 1 n = 178 m3 &lt;- gam(list(EGA ~ grass.perc + as.factor(year) + s(long, lat, bs = &quot;gp&quot;), ~ s(long, lat, bs = &quot;gp&quot;)), family = ziplss(), data = df.train) summary(m3) ## ## Family: ziplss ## Link function: identity identity ## ## Formula: ## EGA ~ grass.perc + as.factor(year) + s(long, lat, bs = &quot;gp&quot;) ## ~s(long, lat, bs = &quot;gp&quot;) ## ## Parametric coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.5612020 0.9360800 -3.804 0.000142 *** ## grass.perc -0.0395195 0.0017325 -22.811 &lt; 2e-16 *** ## as.factor(year)2015 5.5461797 0.9273705 5.981 2.22e-09 *** ## (Intercept).1 0.0008942 0.1018556 0.009 0.992995 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df Chi.sq p-value ## s(long,lat) 30.937 31.330 4819.22 &lt; 2e-16 *** ## s.1(long,lat) 2.485 2.891 15.84 0.00131 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Deviance explained = 78.1% ## -REML = 1761.5 Scale est. = 1 n = 178 newPoints &lt;- st_sample(ks, size = 1000, type = &quot;regular&quot;) %&gt;% as(., &#39;Spatial&#39;) %&gt;% as.data.frame() %&gt;% rename(&quot;long&quot; = &quot;coords.x1&quot;, &quot;lat&quot; = &quot;coords.x2&quot;) %&gt;% cross_join(data.frame(year = as.factor(c(&#39;2014&#39;, &#39;2015&#39;)))) pts.sample &lt;- newPoints coordinates(pts.sample) =~ long + lat proj4string(pts.sample) &lt;- CRS(&quot;+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0&quot;) # Calculate percentage of land area that is grassland withing 5 km of new points location newPoints$grass.perc &lt;- unlist(lapply(extract(rl.nlcd.grass,pts.sample,buffer=5000),mean))*100 # Step 2: obtain predictions newPoints$pred.m1 &lt;- predict(m1, newdata = newPoints, type = &quot;response&quot;) newPoints$pred.m2 &lt;- predict(m2, newdata = newPoints, type = &quot;response&quot;) newPoints$pred.m3 &lt;- predict(m3, newdata = newPoints, type = &quot;response&quot;) 9.6.1 Plot grid predictions # Model 1 ggplot() + geom_tile(data = newPoints, aes(x = long, y = lat, fill = pred.m1))+ labs(title = &quot;Model 1: Abundance of English grain aphids&quot;, x = &quot;Longitude&quot;, y = &quot;Latitude&quot;)+ scale_fill_viridis_c(option = &quot;E&quot;, alpha = 0.95)+ theme(legend.background = element_rect(fill = &quot;transparent&quot;, colour = NA), panel.grid = element_blank(), plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), &quot;lines&quot;), panel.background = element_rect(fill = &quot;grey90&quot;), axis.text = element_text(size = 12), axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1), strip.text = element_text(size = 11, face = &quot;bold&quot;), title = element_text(size = 12, face = &quot;bold&quot;), axis.title = element_text(size = 14))+ facet_wrap(~year) + geom_point(data = data, aes(x = long, y = lat, size = EGA), color = &quot;white&quot;, shape = 21) # Model 2 ggplot() + geom_tile(data = newPoints, aes(x = long, y = lat, fill = pred.m2))+ labs(title = &quot;Model 2: Abundance of English grain aphids&quot;, x = &quot;Longitude&quot;, y = &quot;Latitude&quot;)+ scale_fill_viridis_c(option = &quot;E&quot;, alpha = 0.95)+ theme(legend.background = element_rect(fill = &quot;transparent&quot;, colour = NA), panel.grid = element_blank(), plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), &quot;lines&quot;), panel.background = element_rect(fill = &quot;grey90&quot;), axis.text = element_text(size = 12), axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1), strip.text = element_text(size = 11, face = &quot;bold&quot;), title = element_text(size = 12, face = &quot;bold&quot;), axis.title = element_text(size = 14))+ facet_wrap(~year) + geom_point(data = data, aes(x = long, y = lat, size = EGA), color = &quot;white&quot;, shape = 21) # Model 3 ggplot() + geom_tile(data = newPoints, aes(x = long, y = lat, fill = pred.m3))+ labs(title = &quot;Model 3: Abundance of English grain aphids&quot;, x = &quot;Longitude&quot;, y = &quot;Latitude&quot;)+ scale_fill_viridis_c(option = &quot;E&quot;, alpha = 0.95)+ theme(legend.background = element_rect(fill = &quot;transparent&quot;, colour = NA), panel.grid = element_blank(), plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), &quot;lines&quot;), panel.background = element_rect(fill = &quot;grey90&quot;), axis.text = element_text(size = 12), axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1), strip.text = element_text(size = 11, face = &quot;bold&quot;), title = element_text(size = 12, face = &quot;bold&quot;), axis.title = element_text(size = 14)) + facet_wrap(~year) + geom_point(data = data, aes(x = long, y = lat, size = EGA), color = &quot;white&quot;, shape = 21) 9.7 For the three models you fit in question #3, which model makes the most accurate predictions? How good is the best model in real world terms? Remember we are trying to predict the number of English grain aphids, which is a count! pred.m1 &lt;- predict(m1, newdata = df.test, type = &quot;response&quot;) pred.m2 &lt;- predict(m2, newdata = df.test, type = &quot;response&quot;) pred.m3 &lt;- predict(m3, newdata = df.test, type = &quot;response&quot;) # Calculate AIC AIC(m1, m2, m3) ## df AIC ## m1 34.44604 3764.439 ## m2 14.12373 980.207 ## m3 38.22045 3311.273 # Calculate MAE mae.m1 &lt;- mean(abs(df.test$EGA - pred.m1)) mae.m2 &lt;- mean(abs(df.test$EGA - pred.m2)) mae.m3 &lt;- mean(abs(df.test$EGA - pred.m3)) # Calculate RMSE rmse.m1 &lt;- sqrt(mean((df.test$EGA - pred.m1)^2)) rmse.m2 &lt;- sqrt(mean((df.test$EGA - pred.m2)^2)) rmse.m3 &lt;- sqrt(mean((df.test$EGA - pred.m3)^2)) mae.m1 ## [1] 37.94015 mae.m2 ## [1] 24.29216 mae.m3 ## [1] 35.37633 rmse.m1 ## [1] 99.04343 rmse.m2 ## [1] 77.86172 rmse.m3 ## [1] 95.28032 ggplot(df.test, aes(x = EGA)) + geom_point(aes(y = pred.m1), color = &quot;purple&quot;) + geom_point(aes(y = pred.m2), color = &quot;orange&quot;) + geom_point(aes(y = pred.m3), color = &quot;forestgreen&quot;) + geom_line(aes(y = EGA), linetype = &quot;dashed&quot;, color = &quot;black&quot;) + labs(title = &quot;Observed vs. Predicted EGA&quot;, x = &quot;Observed EGA&quot;, y = &quot;Predicted EGA&quot;) + theme(legend.title = element_blank(), legend.background = element_rect(fill = &quot;transparent&quot;, colour = NA), panel.grid = element_blank(), plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), &quot;lines&quot;), panel.background = element_rect(fill = &quot;grey90&quot;), axis.text = element_text(size = 12), axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1), strip.text = element_text(size = 11, face = &quot;bold&quot;), title = element_text(size = 12, face = &quot;bold&quot;), axis.title = element_text(size = 14)) 9.8 Summarize your results using words, numerical values and figures/maps. Even though the dataset is small, I decided to divide it into train and test sets, using a more conservative split (50/50), to test the prediction accuracy. Overall, the second model, which assumed a negative binomial distribution for the process model, showed superior predictive accuracy. This was evident in the prediction maps and also reflected by the lower AIC, MAE, and RMSE values compared to the other two models. "],["day-1.html", "Chapter 10 Day 1 10.1 Write a paragraph about something new you learned in class within the past 24 hours. 10.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. 10.3 Is there anything else you would like me to be aware of?", " Chapter 10 Day 1 10.1 Write a paragraph about something new you learned in class within the past 24 hours. In today’s class, I learned that spatio-temporal statistics is a relatively new area with few books on it. What’s interesting is that our course is somewhat unique, blending spatial and temporal elements, something not typical in other universities. I was also interetisting to learn about the connection between land-grant universities and the role their statistics departments play in shaping their education systems, especially here at K-State. Plus, I was quite surprised to find out about the earthquakes in southern Kansas, which are likely linked to oil wells. 10.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. So far, I have not struggle in understanding the material covered in class. Everything has been clear. 10.3 Is there anything else you would like me to be aware of? No, there is not anything else until this moment. "],["day-10.html", "Chapter 11 Day 10 11.1 Write a paragraph about something new you learned in class within the past 24 hours. 11.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. 11.3 Is there anything else you would like me to be aware of?", " Chapter 11 Day 10 11.1 Write a paragraph about something new you learned in class within the past 24 hours. In our last class, we discussed the concepts of Geographic Information Systems (GIS). We explored different types of GIS data, including shapefiles, raster images, and points. Shapefiles represent continuous space objects, such as rivers and roads, with defined boundaries. Raster images, which are georeferenced, often come in large file sizes. The planar coordinate system, particularly the Universal Transverse Mercator (UTM), was introduced, emphasizing the importance of converting GIS files to this system for a more accurate representation that accounts for globe distortion. Handling heavy raster files requires caution due to potential computational expenses. The discussion also covered error propagation in GIS data and the conditions under which uncertainty might be disregarded, depending on the project objectives. Additionally, the concept of Gaussian processes and the modeling of spatial data through multivariate normal distributions were briefly introduced, allowing for the generation of random functions, shapes, and points from these distributions. 11.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. I am struggling a little to fully understand the concepts of Gaussian processing and the multivariate normal distribution introduced in class. The explanations provided were somewhat brief, and I am hopeful that future classes will delve deeper into these topics to enhance my understanding. 11.3 Is there anything else you would like me to be aware of? I would like you to be aware that the GIS overview was highly beneficial for aligning our understanding of this important tool. "],["day-11.html", "Chapter 12 Day 11 12.1 Write a paragraph about something new you learned in class within the past 24 hours. 12.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. 12.3 Is there anything else you would like me to be aware of?", " Chapter 12 Day 11 12.1 Write a paragraph about something new you learned in class within the past 24 hours. We started today’s class by reviewing the Gaussian process, which is essentially a PDF that generates random functions. Then, we moved to an introduction of the multivariate normal (MVN) distribution, particularly noting how it is characterized by a mean vector and a covariance matrix, which incorporates a correlation matrix to describe the dependencies among random variables (\\(\\eta \\sim MVN(0,\\sigma^2 R)\\)). We then looked at different structures of correlation matrices, such as compound symmetry and the AR(1) models. These structures are essential for understanding the correlation among sequential data points (over time or space) and their implications for future spatio-temporal applications (such as krigging). The practical application of these concepts was exemplified through a bioluminescence example, where the need to account for spatial random effects in modeling was clearly demonstrated. We compared the two types of correlation matrices covered in class, and to better understand how the addition of one more constraint \\(\\eta\\) in a simple intercept-only linear model (\\(y_i \\sim \\beta_0 +\\eta_i + \\epsilon_i\\)) changes the way the residuals are dispersed across the predictor range. This comparison essentially involved assuming different types of correlation for each data point, using a Gaussian and linear correlation function. The similarities to machine-learning were also highlighted, where the “phi” parameter of the function would be named as “tuning” parameter. We concluded the class by obtaining a much more representative model for bioluminescence at different depths and comparing the levels of wiggliness according to different correlation functions. 12.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. It was not very clear to me how to effectively implement different types of correlation matrices, particularly in the context of incorporating variables that could explain the spatial and temporal influences on our data. I believe that one more example will help to clarify this. 12.3 Is there anything else you would like me to be aware of? Even though it’s sometimes not very worth it to go so deep into the specifics of functions or mathematical formulas, a brief explanation is always very helpful for us. For example, when you explained the \\(\\phi\\) parameter of the Gaussian function. "],["day-12.html", "Chapter 13 Day 12 13.1 Write a paragraph about something new you learned in class within the past 24 hours. 13.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. 13.3 Is there anything else you would like me to be aware of?", " Chapter 13 Day 12 13.1 Write a paragraph about something new you learned in class within the past 24 hours. In yesterday class, we had an overview of how to build linear models to account for correlated errors by adding a spatial random effect, denoted as \\(\\eta\\), to model the variability in rainfall across different locations (\\(y_i = \\beta_0 + \\eta_i\\), where \\(\\eta_i = (\\eta_1, \\eta_2, ... \\eta_n) \\sim MVN(0, \\sigma^2C)\\)). This approach allows for the incorporation of spatial dependencies into our models, adding a spatial random effect to each of the measured depths. We then moved to the hierarchical modeling framework. We assumed the data model as \\([z|y...] = N(y, \\sigma^2_z)\\), and the process model as \\([y|\\beta_0, \\phi, \\sigma^2] = MVN(\\beta_0, \\sigma^2C)\\). Until this point, the expected value is a constant (\\(\\beta_0\\)), meaning we are considering the same rainfall across all weather stations, which would be our simplest model. To better exemplify the theory and concepts, we proceeded with a live example in R. We started by conducting an exploratory analysis of the data and transforming the coordinate system to planar (UTM-13). Then, we built three different models: an intercept-only model (\\(y_i = \\beta_0 + \\epsilon_i\\) with \\(\\epsilon_i \\sim N(0, \\sigma^2)\\)), a second-order polynomial model (here, we incorporate the spatial component), and the hierarchical model that was previously outlined. We checked the predictions of all models in raster format. The first model-based estimate showed us that the maximum precipitation was about 3.99”, the polynomial model suggested 5”, and the most accurate one, which was the hierarchical model, resulted in 9.76” as the maximum precipitation value. The more assumptions and constraints considering our knowledge about precipitation improved our ability to make inferences and also to obtain a more realistic model. 13.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. The last class was one of the best for me. Everything was very clear! The only part I may struggle with is understanding the syntax of the gls() function in R. As pointed out in class, it may require more experience to learn how to build a model effectively. 13.3 Is there anything else you would like me to be aware of? The live examples really help us connect the theory with the real problem we are trying to solve. "],["day-13.html", "Chapter 14 Day 13 14.1 Write a paragraph about something new you learned in class within the past 24 hours. 14.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. 14.3 Is there anything else you would like me to be aware of?", " Chapter 14 Day 13 14.1 Write a paragraph about something new you learned in class within the past 24 hours. In yesterday’s class, we had the chance to discuss Activity 2 and the final project. It was interesting to see and hear the different approaches other students took to solve Activity 2, and the different data that were collected (konza, field, etc). 14.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. I do not fully understand all the steps for the kriging process. Some packages help to work on the problem but still there are many arguments that can be adjusted. 14.3 Is there anything else you would like me to be aware of? "],["day-14.html", "Chapter 15 Day 14 15.1 Write a paragraph about something new you learned in class within the past 24 hours. 15.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. 15.3 Is there anything else you would like me to be aware of?", " Chapter 15 Day 14 15.1 Write a paragraph about something new you learned in class within the past 24 hours. In yesterday’s class, we had another opportunity to discuss Activity 2 and the final project. At the beginning of the class, we were introduced to various alternatives for addressing the elevation problem. These included the low-rank Gaussian process, regression trees (which involve a series of flat elevation spots), gradient boosting (noted for its high predictive accuracy), and support vector machines (which helps to smooth the predictions). 15.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. I would like to gain a better understanding of how gradient boosting works and how it can be superior to random forests, for example. 15.3 Is there anything else you would like me to be aware of? "],["day-15.html", "Chapter 16 Day 15 16.1 Write a paragraph about something new you learned in class within the past 24 hours. 16.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. 16.3 Is there anything else you would like me to be aware of?", " Chapter 16 Day 15 16.1 Write a paragraph about something new you learned in class within the past 24 hours. During the last class, we briefly reviewed what was taught before the spring break. During the spring break week, we had the chance to work on the final project and Activity 2, where we were introduced to some alternative, though not entirely “new” methods of dealing with our problem. We discussed the framework of model building, which first involves setting our goals, then writing out our statistical model, and finally programming and analyzing given data. In a live example, we reviewed linear models with correlated errors for the Kansas precipitation example, and finally, we were introduced to generalized additive models (GAMs). In the live example, we first assumed a normal distribution for the data model, and we also tested the Tweedie distribution, which allows for positive real numbers but also the inclusion of zeros. We ended up concluding that using a GAM model with the Tweedie distribution was the best approach, with our assumptions leading to better estimates, including precipitation greater than zero and more similar values to the observed data. 16.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. Even though it would take time, digging a bit more into the GAM syntax would be awesome so we can understand better how to fix “phi,” for example, or how to define the correct number of knots. I mean, something general but that could give us more applied skills. 16.3 Is there anything else you would like me to be aware of? It was interesting and useful to see how your workflow is, I mean, always reviewing the code and the part of data memorization, which I also think is a core skill. "],["day-16.html", "Chapter 17 Day 16 17.1 Write a paragraph about something new you learned in class within the past 24 hours. 17.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. 17.3 Is there anything else you would like me to be aware of?", " Chapter 17 Day 16 17.1 Write a paragraph about something new you learned in class within the past 24 hours. During the last class, I worked on Activity 2 and implemented one of the Generalized Additive Models (GAM) to predict elevation. Additionally, I had the opportunity to discuss the final project with Aidan. 17.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. I find myself getting somewhat lost to write out the more complex models, such as gradient boosting and random forests. 17.3 Is there anything else you would like me to be aware of? The work days are very beneficial for us, as they allow us to put into practice what we learn in class. "],["day-17.html", "Chapter 18 Day 17 18.1 Write a paragraph about something new you learned in class within the past 24 hours. 18.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. 18.3 Is there anything else you would like me to be aware of?", " Chapter 18 Day 17 18.1 Write a paragraph about something new you learned in class within the past 24 hours. We began the class by reviewing how to choose a distribution necessary to support the data. It’s important to be aware of various distributions. We also discussed model checking and evaluation. There is no correct method for model checking as it is very application-specific. We started with a model that includes only an intercept, assuming that errors are normally distributed. We examined predictions and scoring rules: logarithmic (the higher the number, the better); MSE (mean squared error), which should be used for normally distributed data; and MAE (mean absolute error), the most interpretive. The way we fit the model and our assumptions is related to the best or more appropriate scoring rule. We then looked at prediction intervals and the calibration of the model (proportion of predictions that fall within the 95% prediction interval). The issue was that the prediction intervals were too wide. If all our assumptions had been correct, the model would have been calibrated. Therefore, we fit a polynomial model, which somewhat assumes that the terrain is not flat (bowl shape). This model improved prediction accuracy (logarithmic, MSE, MAE) but had a calibration of 0.83 because the prediction intervals were now too narrow, indicating that the assumptions may not be the best yet. Next, we tried kriging, which showed worse or similar predictive accuracy compared to the polynomial model, and obtaining prediction intervals was much more challenging. We then tried boosted regression trees, which had roughly the same accuracy as the polynomial model. However, as a machine learning tool, it does not provide prediction intervals. The differences between the methods are so fine that, if we are concerned with model checking, we would need significantly more data. 18.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. I was not very familiar with the calibration technique, nor the logarithmic scoring rule. I did not completely understand and will read the reference paper provided. 18.3 Is there anything else you would like me to be aware of? "],["day-18.html", "Chapter 19 Day 18 19.1 Write a paragraph about something new you learned in class within the past 24 hours. 19.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. 19.3 Is there anything else you would like me to be aware of?", " Chapter 19 Day 18 19.1 Write a paragraph about something new you learned in class within the past 24 hours. In the last class, we briefly saw some model selection criteria (AIC, BIC, …), and then we worked on the final projects. 19.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. In the last class, I was mostly working on the final project and didn’t have anything I struggled to follow. 19.3 Is there anything else you would like me to be aware of? "],["day-19.html", "Chapter 20 Day 19 20.1 Write a paragraph about something new you learned in class within the past 24 hours. 20.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. 20.3 Is there anything else you would like me to be aware of?", " Chapter 20 Day 19 20.1 Write a paragraph about something new you learned in class within the past 24 hours. In today’s class, we were introduced to a spatial-temporal disease dataset, along with a tutorial written years ago, which remains very useful today. We first loaded the packages sf, sp, and raster, as well as the dataset. We then prepared a point coordinate dataframe to visualize the map of the raw data, indicating the amount of aphids at given latitudes and longitudes. We checked the goals set for the study, which were to create a map with aphid count predictions in Kansas and to identify which environmental factors influence the number of aphids. We downloaded data from the National Land Cover Database, separating grassland (pixel value = 1) from non-grassland (pixel value = 0), and looked at some summaries of the data. We fit three different models to the data. The first assumes a data model where \\(Z = y\\) and the process model is \\([y|.]\\). For the three models, we applied a log-link function to the expected value \\(E(y) = e^{\\beta_0 + \\beta_1 \\times X + \\eta_s + \\eta_t}\\), where \\(\\eta_s\\) is the site effect and \\(\\eta_t\\) is the time effect, ensuring the expected value is always greater than zero. The process model for the spatial effect is \\(\\eta_s \\sim MVN(0,\\Sigma)\\), and for the temporal effect, given we have only two years of data, we used a more crude approximation: \\(\\eta_t = \\alpha_{2014}\\) if \\(t=2014\\) and \\(\\eta_t = \\alpha_{2015}\\) if \\(t=2015\\). The first model is assumed to follow a Poisson distribution, the second a negative binomial distribution, and the third model a zero-inflated Poisson distribution (a mixture distribution). Next class: how to present results and conduct model checking. 20.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. I did not really understand the R output of the model with the mixture distribution (zero-inflated Poisson distribution). 20.3 Is there anything else you would like me to be aware of? The new example was really good for applying all that we have learned, and the quick explanation about the link functions was very useful. "],["day-2.html", "Chapter 21 Day 2 21.1 Write a paragraph about something new you learned in class within the past 24 hours. 21.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. 21.3 Is there anything else you would like me to be aware of?", " Chapter 21 Day 2 21.1 Write a paragraph about something new you learned in class within the past 24 hours. In today’s class, I learned about the importance of checking the credentials of R package developers before deciding to use their packages for long-term projects. It was interesting to learn that increasing the volume of data to millions or billions of data points per second doesn’t necessarily help solve the problem; it might actually introduce more measurement error as the intervals between recorded locations get smaller. The class discussion also touched on how overfitting is often viewed as a more significant issue than underfitting by peers reviewing your model or work. However, we saw that a high R-squared value could misleadingly suggest an underfitted predictive model that would not be suitable for navigation (if that was out goal). The idea that regression trees could be an alternative modeling approach was unexpected. The model was set in R to overfit the data (complexity parameter), and although they didn’t perform perfectly due to the continuous nature of time and the assumption that we were stationary for several seconds, the resulting shape was still pretty impressive. What I take from this is that the right approach depends on the specific question we are trying to answer. 21.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. I believe that I understood all the material covered in class today. 21.3 Is there anything else you would like me to be aware of? I really enjoyed this second class!! "],["day-20.html", "Chapter 22 Day 20 22.1 Write a paragraph about something new you learned in class within the past 24 hours. 22.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. 22.3 Is there anything else you would like me to be aware of?", " Chapter 22 Day 20 22.1 Write a paragraph about something new you learned in class within the past 24 hours. This class focused on the final project, providing me the opportunity to discuss priors for non-linear models in agriculture, discuss model convergence, and also the application of the area under the curve (AUC) for non-linear models. 22.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. I did not have any struggle in the last class. 22.3 Is there anything else you would like me to be aware of? It was good to know that my plans for the final project are on the right track. I am making progress and planning to start writing the tutorial and report soon. "],["day-21.html", "Chapter 23 Day 21 23.1 Write a paragraph about something new you learned in class within the past 24 hours. 23.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. 23.3 Is there anything else you would like me to be aware of?", " Chapter 23 Day 21 23.1 Write a paragraph about something new you learned in class within the past 24 hours. In today’s class, we looked into an aphid disease dataset in Kansas (KS). The goals are to produce accurate prediction maps for vector abundance and the probability of infection, and to understand the environmental factors influencing vector abundance and the probability of infection. Auxiliary data such as weather, land cover, and crop data may be very helpful. In the live example, we loaded the disease dataset and a raster with the land cover in KS. We then transformed the raster to show the grassland area only and calculated and plotted the percentage of land area that is grassland within a 5km radius at sampled locations. Moving to the modeling, we assume that the data model is \\(z = y\\) (true abundance of aphids is the same as the counted), and that the process model is \\([y|\\ldots]\\) (we assume a distribution). The expected value of the process model was assumed as: \\(E(y) = e^{\\beta_0+\\beta_1 \\times X + \\eta_s + \\eta_t}\\) (log link function), with \\(\\eta_s \\sim MVN(0,\\Sigma)\\). We assumed three different distributions for the process model (for \\(y\\)) which were: Poisson, negative binomial, and a zero-inflated Poisson distributions. We don’t care about the significance, but we do care about the magnitude of the estimates according to changes in grassland. We generally see that if grassland increases, the abundance of aphids decreases. Checking the coefficients, if there is less grassland, the first model says we would have twice the number of aphids, the second model 35% more, and the third model 33% more aphids. We finally compare models using AIC to check the predictive performance, and by far, the negative binomial model was superior to the other models. We also checked concurvity (effect of random effects on fixed effects - we cannot infer if there is or is not a correlation between the grassland (\\(X\\)) or the \\(\\eta_s\\)). 23.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. I did not fully understand the concept of concurvity. 23.3 Is there anything else you would like me to be aware of? The discussion and paper regarding whether or not to include random effects in a model was very interesting. "],["day-22.html", "Chapter 24 Day 22 24.1 Write a paragraph about something new you learned in class within the past 24 hours. 24.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. 24.3 Is there anything else you would like me to be aware of?", " Chapter 24 Day 22 24.1 Write a paragraph about something new you learned in class within the past 24 hours. We started the class by looking at coefficient estimates of aphids in grassland, focusing only on its effect size and uncertainty interpretation. Since our main concern is predictive performance and we aim to retain all available data, the AIC becomes a efficient metric for evaluating model predictive accuracy. Additionally, we discussed concurvity, which is similar to the concept of collinearity, with values above 0.6 indicating potential issues (similar to collinearity). We then conducted an exercise to remove the spatial effect from the first GAM model, observing that grassland appeared to be in competition with the spatial random effect. Upon removing the spatial random effect, the coefficient’s effect size almost doubled and became more certain. Semivariograms of residuals revealed traces of covariates that could improve the models. Furthermore, through the assessment of predicted zeros, negative binomial models demonstrated superiority over Poisson and zero-inflated models. The next dataset comprised the proportion of infected (tested positive) aphids. Here, our objective moves from explaining aphid counts to determining the number of positive aphids given the number tested (assuming no false-positives - highly accurate test). We assume the data model \\(y = z\\), and the process model \\(y_i \\sim Bin(N_I,P_i)\\), with a logit-link function \\(logit(P_i) = \\beta_0+\\beta_1\\times X + \\eta_s + \\eta_t\\). Our focus lies in inferring the probability of disease presence and understanding the influence of grasslands. However, we found an issue in the dataset known as complete separation, occurring when there are very few positives or negatives. Our models might be overly complex with too many parameters, and we lack sufficient information to retain spatial and time effects in the model. 24.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. Last class was very clear to me, and I do not have any current struggle. 24.3 Is there anything else you would like me to be aware of? It would be nice to explore a Bayesian model version for this new dataset, even if we do not delve deeply into the theory. "],["day-23.html", "Chapter 25 Day 23 25.1 Write a paragraph about something new you learned in class within the past 24 hours. 25.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. 25.3 Is there anything else you would like me to be aware of?", " Chapter 25 Day 23 25.1 Write a paragraph about something new you learned in class within the past 24 hours. Last class I worked on the Activity 3, trying to play with other land cover types and variables that could explain the aphid abundance. 25.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. Last class was very clear to me, and I do not have any current struggle. 25.3 Is there anything else you would like me to be aware of? For the next class, it would be nice to explore a Bayesian model version for the Activity 3. "],["day-24.html", "Chapter 26 Day 24 26.1 Write a paragraph about something new you learned in class within the past 24 hours. 26.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. 26.3 Is there anything else you would like me to be aware of?", " Chapter 26 Day 24 26.1 Write a paragraph about something new you learned in class within the past 24 hours. In the last class, we analyzed the earthquake dataset in Kansas, which comprised its time, location, and magnitude. In the first approach, we utilized well maps to make predictions and inferences, using this information as a covariate to explain increases or decreases in the risk of an earthquake. In the first hierarchical model, we assumed the data model as \\(y = z\\) and a process model \\(z \\sim IPPP(\\lambda_{(s,t)})\\), being the deterministic equation of \\(\\lambda_{(s,t)} = \\exp^{\\beta_0+\\beta_1 \\times X_{(s,t)} + \\eta_s + \\eta_t}\\). The \\(\\eta_s\\) and \\(\\eta_t\\) were assumed to follow a \\(MVN\\) distribution with mean 0 and covariance \\(\\Sigma\\). We then calculated the distances from earthquakes to the nearest wells, which averaged 7.5 km. In a subsequent step, we generated 25k random points across Kansas and checked the average distance of all points to the nearest earthquake, which was 10.5 km. This crude summary indicates that earthquakes occur 3km further away from random points than from wells. The model coefficients provided information about the intensity of the earthquakes, which declined as the distance increased. Lastly, we mapped \\(\\lambda_{(s,t)}\\) and examined its predictions. As the second approach, we fit a log-Gaussian Cox process model, which ultimately showed that space and time significantly influence earthquakes and that wells did not completely explain earthquake intensity in Kansas. 26.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. The live example was very clear. However, I was somewhat confused towards the end of the class when we compared the real space-time effect of the second approach with the first one, using wells as a covariate. I suppose the point of the comparison was to demonstrate that these factors were not 100% related to the locations of oil and gas lines. 26.3 Is there anything else you would like me to be aware of? "],["day-25.html", "Chapter 27 Day 25 27.1 Write a paragraph about something new you learned in class within the past 24 hours. 27.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. 27.3 Is there anything else you would like me to be aware of?", " Chapter 27 Day 25 27.1 Write a paragraph about something new you learned in class within the past 24 hours. Last class I worked on the final project and hope to have presentation ready by Wednesday for the final presentation (: In the meantime I will also look for a peer-reviewer. 27.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. Last class was very clear to me, and I do not have any current struggle. 27.3 Is there anything else you would like me to be aware of? "],["day-26.html", "Chapter 28 Day 26 28.1 Write a paragraph about something new you learned in class within the past 24 hours. 28.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. 28.3 Is there anything else you would like me to be aware of?", " Chapter 28 Day 26 28.1 Write a paragraph about something new you learned in class within the past 24 hours. We first visualized all earthquakes since 1977, focusing on the temporal effects only (no spatial considerations), and the locations of wells in Kansas. We then extrapolated the number of earthquakes to a specific area (100 \\(km^2\\)). Predicting the intensity of earthquakes in Kansas proved challenging. We also forecasted the expected number of earthquakes in a 1,000 \\(km^2\\) area for the year 2024. We discussed the impact of adding or removing wells in a given region using a regression-style approach, which allows us to manipulate these variables as needed. Lastly, we reviewed the white-nose bat infection dataset in the US. 28.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. I don’t fully comprehend the partial differential equations, although I managed to follow the example discussed in class. 28.3 Is there anything else you would like me to be aware of? "],["day-27.html", "Chapter 29 Day 27 29.1 Write a paragraph about something new you learned in class within the past 24 hours. 29.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. 29.3 Is there anything else you would like me to be aware of?", " Chapter 29 Day 27 29.1 Write a paragraph about something new you learned in class within the past 24 hours. Last class I worked on the final project and tutorial. 29.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. Last class was very clear to me, and I do not have any current struggle. 29.3 Is there anything else you would like me to be aware of? "],["day-3.html", "Chapter 30 Day 3 30.1 Write a paragraph about something new you learned in class within the past 24 hours. 30.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. 30.3 Is there anything else you would like me to be aware of?", " Chapter 30 Day 3 30.1 Write a paragraph about something new you learned in class within the past 24 hours. In the marathon example, we saw that checking model performance might require auxiliary information, and replicating spatio-temporal data is almost impossible. Instead, we would need to explore alternative methods, such as using a second device to record position. The gold standard way to do it would be to achieve an excellent performance in a dataset never ever seen by the model (out-of-sample predictive performance). The discussion also covered model improvement, emphasizing that our assumptions significantly influence this aspect. I was unaware of how to fit a model using recursive partitioning, where a statistical model is fitted for each terminal node, nor did I know that this was a cutting-edge technique. Although I had heard about it, it is not commonly used in my area of study. The definition of overfitting was also presented in a straightforward way (e.g.: interpolation, going through every single data point) where we would capture not only the true location but also the data associated error. In conclusion, a model is a simplification of reality, simpler than the actual data but still generative (capable of producing fake data). I particularly liked the definition of statistical models as a combination of a deterministic equation and a probabilistic function (error term). However, we need to be aware that sometimes a more complex model may complicate our lives rather than help depending on what question we are trying to answer. 30.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. I am kind of struggling a bit to translate what we learned from the marathon example to my research area. I mean, everything seems pretty doable, but at the same time, it’s very complex to implement, depending on the type of data. This is despite the fact that almost all data has a temporal or spatial component, but not all will have this “visual” interpretation. 30.3 Is there anything else you would like me to be aware of? I have been reading the reference book, and there are some examples that directly or indirectly relate to topics discussed in class (e.g.: IDW being simply a weighted average of the data points and therefore deterministic). This connection really helps connecting the lecture and the material. If possible, it would be great to have a brief explanation of the GAM models in class (considering how customizable these models are), approached in a more applied manner, more on how to select the smooth, determine the number of knots, or choose the function. "],["day-4.html", "Chapter 31 Day 4 31.1 Write a paragraph about something new you learned in class within the past 24 hours. 31.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. 31.3 Is there anything else you would like me to be aware of?", " Chapter 31 Day 4 31.1 Write a paragraph about something new you learned in class within the past 24 hours. We began the class by discussing outlier removal and other forms of data “manipulation.” As taught, it’s generally not advisable because establishing criteria for “editing” data is challenging, and it may lead to loss of information or even introduce problems, as highlighted in the Challenger temperature example. We then discussed some theoretical concepts: prediction (which is usually more accurate within the interval of collected data), hindcast (determining probable past values), and forecast (future scenarios of the data). It’s essential to communicate the uncertainty surrounding the predictions. However, the method used to inform uncertainty can influence the perception of a model. For instance, confidence intervals provide uncertainty about the expected values, often narrowing the visual uncertainty and, in most of the cases, failing to meet assumptions of a frequentist model. In contrast, we saw that prediction intervals might offer a more reliable measure of the uncertainty of future collected data, with a given probability, based on what has already been observed. We also discussed the inference of observable (directly measurable in the real-world) and unobservable quantities (like the slope of a regression line, its magnitude, direction, and certainty). It was very helpful to revisit vector and matrix notations, as well as operations. We started reviewing distribution theory, and it was interesting to learn that during your time in Colorado, knowledge in this area significantly distinguished the proficiency of you co-workers in statistics, emphasizing the importance of this topic. 31.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. I feel that I still somewhat struggle with, or am not entirely confident in, formulating more complex statistical models or fully understanding all the components of mathematical equations related to probability theory. It’s a topic I’ve been trying to improve if I need to implement in my research. While it’s usually easier to implement some of these concepts computationally in R, writing them out and comprehending the entire mathematical foundation behind them is more challenging. 31.3 Is there anything else you would like me to be aware of? You briefly mentioned experimental data and the assumption that spatio-temporal effects shouldn’t influence the outcome, yet they sometimes do. I would appreciate it if you could dig a bit more into this topic and provide examples when possible. Additionally, it would be beneficial if we had another opportunity to find a partner to work on the final project. "],["day-5.html", "Chapter 32 Day 5 32.1 Write a paragraph about something new you learned in class within the past 24 hours. 32.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. 32.3 Is there anything else you would like me to be aware of?", " Chapter 32 Day 5 32.1 Write a paragraph about something new you learned in class within the past 24 hours. In today’s class we reviewed some concepts of what is a statistical model and how it generates data similar to what was observed based on our assumptions. Then we moved to probability density theory and some examples were given for the normal and Bernoulli distributions (special case of binomial) and as shown, we need to be careful on how we write out the models to communicate what we want to show. We also learned about moments, with the first moment being the expected value and the second central moment being the variance, represented as \\(y_i= N(\\mu_i, \\sigma^2_i)\\). We saw how to control these moments mathematically, with the first moment \\(\\mu_i\\) being controlled by \\(\\beta_0 + \\beta_1 \\times X_i\\) and the second moment \\(\\sigma^2\\) being controlled by the exponential function \\(\\exp^{\\alpha_0 + \\alpha_i \\times X_i}\\). We learned that the expected value, calculated as an integral, is different from the mean, which is just the average of numbers. The line of best fit in the marathon example we looked at was actually showing the expected value, not just a “visual representation”. We discussed different estimation methods, like maximum-likelihood estimation, which was created in the 1930s and gives unbiased, precise estimates, and least squares, which is used only for estimation and doesn’t allow for making inferences. We also talked about the inverse probability integral transform, a method that uses a uniform distribution to create a random variable and then changes it into another distribution. This is what computers do to generate random numbers for other PDFs. Lastly, we briefly discussed about about MCMC, an algorithm that lets us create random variables from almost any distribution and sample from those. 32.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. During the last part of our class, we discussed about difference equations and differential equations, which was quite challenging for me. I understand the basic concept that difference equations deal with discrete time intervals, while differential equations are used for dynamical systems, treating time as continuous. However, my understanding doesn’t go much beyond this. Since I have never had a class specifically on these topics. I also didn’t really understood when we would want to have bias or unbiased estimates using maximum-likelihood. The discussion on bias and unbiased estimates in the context of maximum-likelihood was a bit confusing. I didn’t quite get when we would prefer to have biased estimates. 32.3 Is there anything else you would like me to be aware of? The idea of coding these functions from scratch as an exercise seemed like a great way to understand these concepts better. "],["day-6.html", "Chapter 33 Day 6 33.1 Write a paragraph about something new you learned in class within the past 24 hours. 33.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. 33.3 Is there anything else you would like me to be aware of?", " Chapter 33 Day 6 33.1 Write a paragraph about something new you learned in class within the past 24 hours. In today’s class, we reviewed concepts of difference, differential, and partial differential equations. Initially, these concepts seemed more challenging, but with examples from the last class, I now feel more comfortable learning about them. We then briefly discussed agent-based models, which resemble the rules of video games or building evacuation plans, and tend to be used without data. Next, we started a motivating data example concerning whooping cranes. Data on the whooping cranes were collected in their Texan habitats, described as “giant parking lots” or “little bombing ranges.” These habitats exhibit interesting behavioral patterns, such as siblings killing each other and older individuals migrating more efficiently. We recognized that the data could be seen as an aggregation of spatio-temporal data, reflective of the data generation process. The goal is to predict when the population will exceed 1000 individuals. We discussed hierarchical models, which can be empirical or Bayesian. The empirical model simplifies the process by removing the parameter layer and utilizing maximum-likelihood estimation. The Bayesian hierarchical model incorporates Bayes’ theorem for estimation, which applies prior knowledge to statistical models and inference. In these models, a random variable \\(z\\) is conditional on another variable \\(y\\), representing the data we wish to have, such as the actual number of whooping cranes in the population. Hierarchical models acknowledge that all data is collected with some level of measurement error. The data model, in this context, is the actual data we have, conditional on the data we wish we had, along with some parameters. The process model is a statistical representation of the data we aim to collect, for example, the true number of whooping cranes. This is complemented by the parameter model, which is the prior model. Bayesian statistics allow us to “shut off” our brains once the models, distributions, and mathematical framework are defined; the computational tasks can then be managed by algorithms. This approach yields the posterior distribution, which reflects our updated beliefs about the parameters after considering the data. Bayes’ rule is a technique used to determine the posterior distribution of the parameters, given the data. Lastly, the posterior predictive distribution provides a probabilistic forecast based on this updated information. 33.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. Towards the end of the class, we didn’t discuss much about the parameter model. I think we missed a thorough explanation, as we had for the data and process models. Also, regarding the giant fish example, the process of grouping species was not very clear to me. The rest of the class was pretty clear. We reviewed some concepts at the beginning of the class, the whooping crane dataset was clearly explained, as well as the hierarchical Bayesian model structure. 33.3 Is there anything else you would like me to be aware of? It was one of the best classes so far, and the way you described the whooping crane dataset was hilarious. I will never forget the comment that Texas is a giant parking lot haha. "],["day-7.html", "Chapter 34 Day 7 34.1 Write a paragraph about something new you learned in class within the past 24 hours. 34.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. 34.3 Is there anything else you would like me to be aware of?", " Chapter 34 Day 7 34.1 Write a paragraph about something new you learned in class within the past 24 hours. In today’s class, we went through the framework of hierarchical modeling using the whooping crane dataset. More specifically, we discussed the data and process models. The data model (model for the data that was generated or collected) can be defined as \\([z|y,\\theta_D]\\), where \\(\\textbf{z}\\) represents the counts put into a vector (actual data), \\(\\textbf{y}\\) is the true number of whooping cranes, \\(\\theta_D\\) is just a parameter (probability of a whooping crane to be there), and \\([~]\\) is the notation used to indicate a probability density function (PDF) or probability mass function (PMF). Then, we started a discussion on what would be the support of \\(\\textbf{y}\\) and \\(\\textbf{z}\\), which will essentially ensure predictions within the range that the data distribution can take. Considering that counts are integers, the lower boundary should not be lower than 0, and the upper bound is somewhat unknown (we could assume infinity). We ended up assuming that the data model follows a Binomial distribution (\\([z|y,p] = \\text{Binomial}(y,p)\\)). The second component of the hierarchical model is the process model, representing the data we wish we had. In this component, a PDF given \\(y\\) could have generated the true number of whooping cranes, \\([y|\\theta_P]\\). In this example, we assumed a Poisson distribution for the process model, where lambda is the expected value and variance (\\([y|\\lambda_0,\\gamma] = \\text{Pois}(\\lambda_0(t))\\)). We then defined the mathematical equation for lambda as \\(\\lambda_t = \\exp^{\\beta_0+\\beta_1 \\times t}\\), which is the analytical solution of an ordinary equation to describe growth rate (differential equation). In this equation, the true number of whooping cranes depends on the initial number and the growth rate. Lastly, we started looking at the parameter model, which is an extra layer to build a hierarchical Bayesian model, defining priors to parameters \\(p\\), \\(\\lambda_0\\), and \\(\\gamma\\). 34.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. I am struggling with some of the mathematical operations of the differential equations, or visualizing how those empirical hierarchical models would be implemented/useful in practice without the Bayesian component. The rest of the class was very clear to me! 34.3 Is there anything else you would like me to be aware of? The idea of having a Q&amp;A session in class would be nice. It won’t be the same format, but I remember that I learned a lot just by listening to other people questions during the office hours at the STAT705 course you taught. Especially for international students, sometimes it is hard to us to formulate a straightforward and concise question. Great idea! "],["day-8.html", "Chapter 35 Day 8 35.1 Write a paragraph about something new you learned in class within the past 24 hours. 35.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. 35.3 Is there anything else you would like me to be aware of?", " Chapter 35 Day 8 35.1 Write a paragraph about something new you learned in class within the past 24 hours. In today’s class, we reviewed the hierarchical Bayesian structure and also defined the parameter model (distributions and hyperparameters). Data model: \\([ \\underline{z} |\\underline{y},p]=\\text{binomial}(\\underline{y},p)\\) + Process model: \\([y_t|\\gamma, \\lambda_t] = \\text{Pois}(\\lambda_t)\\), \\(\\lambda_t=\\lambda_0e^{\\gamma(t-t_0)}\\) + Parameter model: \\([\\gamma]=\\text{uni}(0,0.1)\\), \\([\\lambda_0]=\\text{duni}(2,50)\\), \\([p]=\\text{unif}(0,1)\\) Then, we discussed that models are never perfect, but we can specify them and make assumptions to be more fair in terms of representing “reality”. We had a brief introduction and tutorial in R on how the composition samplers work (such as MCMC), how sample rejection works, and how this can make an algorithm more efficient. As we saw, by simulating the whooping crane data using a composition sampler, we can sometimes approximate the true data. More specifically, using a for loop function in R, we draw samples for \\(\\gamma\\) from a uniform distribution, and for \\(\\lambda_0\\) from a discrete uniform. Then we plug these two values into the mathematical equation for the process model, and plug in the calculated \\(\\lambda\\) to the process model, we draw samples from a Poisson distribution. Lastly, we obtain a sample for the \\(p\\) parameter of the data model from a uniform distribution, and using draw samples from the process model we generate \\(y\\) from a Binomial distribution with probability \\(p\\). Then we plot a few simulations to compare how the composition sampler is variable, and also we visualize the mean of each random variable using the posterior distributions. 35.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. It would be beneficial to receive a clearer explanation of how MCMC enhances the efficiency of composition sampling. Overall, the class was very clear, and the live tutorial greatly helped in clarifying the hierarchical Bayesian framework. 35.3 Is there anything else you would like me to be aware of? The live examples are the best way for me to assimilate theory and its real-world application. This was the best class so far!! "],["day-9.html", "Chapter 36 Day 9 36.1 Write a paragraph about something new you learned in class within the past 24 hours. 36.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. 36.3 Is there anything else you would like me to be aware of?", " Chapter 36 Day 9 36.1 Write a paragraph about something new you learned in class within the past 24 hours. In today’s class, we were introduced to the next activity which included an example of a DEM map created with a LiDAR sensor. We also explored a data collected in a parking lot at K-State. Surprisingly, there was an unexpected elevation range with a difference of 9 feet. We then discussed the goal in space-time statistics of building custom statistical models. The hierarchical structure was briefly reviewed, from selecting probability density functions (PDFs) to modeling layers, choosing algorithms, and performing statistical inference. Important skills for this process were emphasized, such as setting clear goals, selecting appropriate statistical methods, and choosing a statistical programming language for data analysis. We also looked at a new example that involves predicting and quantifying rainfall uncertainty in Kansas. The initial steps in R covered data acquisition, selecting the area of study, and conducting exploratory data analysis, which included adjusting the sizes of weather station markers based on rainfall amounts and addressing missing data by case deleting NA’s. 36.2 Write a paragraph about something you are struggling to understand that was covered in class within the past 24 hours. Since this was an introductory class to a new example, I found it quite clear. However, I was somewhat confused towards the end of the class during the discussion about potential issues arising from different assumptions about missing data (NA’s). 36.3 Is there anything else you would like me to be aware of? Despite the occasional repetition, revisiting some concepts from previous classes has been great to keep it fresh in our minds. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
